# 数据曲线

阿里云实时计算提供了当前作业的核心指标概览页面。您可以通过数据曲线对作业的运行情况进行一键式的诊断。实时计算未来还会提供更多基于作业现状的深度智能分析算法，辅助您进行智能化和自动化诊断。

数据曲线示例如下。

![Curve Charts](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/6321659951/p33954.png)

**说明：**

-   实时计算作业只有在**运行**状态下才显示数据曲线指标，**暂停**和**停止**状态均不显示数据曲线指标。
-   作业指标是实时计算系统异步后台采集，存在一定延迟。作业启动1分钟后，开始逐步采集各项指标，并在数据曲线显示。

## 登录数据曲线页面

1.  登录**作业运维**页面。
    1.  登录[实时计算控制台](https://stream-ap-southeast-3.console.aliyun.com)。
    2.  单击页面顶部的**运维**。
    3.  在**作业列表**区域，单击**作业名称**下的目标作业名。
2.  在**作业运维**页面，单击页面顶部的**数据曲线**。

## OverView

-   **Failover**

    Failover曲线显示当前Job出现Failover（错误或异常）的频率。计算方法为当前Failover时间点的前1分钟内出现Failover的累计次数除以60。例如，最近1分钟Failover一次，Failover的值为1/60=0.01667。

-   **延时**

    为了全面的了解实时计算全链路的时效状况和作业的性能，实时计算提供3种延时指标：

    -   **业务延时**（Processing Delay）：业务延迟=当前系统时间-当前系统处理的最后一条数据的事件时间（Event time）。如果后续没有数据再进入上游存储，由于当前系统时间在不断往前推进，业务延时也会随之逐渐增大。
    -   **数据滞留时间**（Data Pending Time）：数据滞留时间=数据进入实时计算的时间-数据事件时间（Event time）。即使后续没有数据再进入上游存储，数据滞留时间也不会随之逐渐增大。通常用数据滞留时间来评估当前实时计算作业是否存在反压。
    -   **数据间隔时间**（Data Arrival Interval）：数据间隔时间=业务延迟-数据滞留时间。当实时计算没有反压时，数据滞留时间较小且平稳，数据间隔时间可以反映数据源数据间的稀疏程度。当实时计算存在反压时，数据滞留时间较大或不平稳，此参数没有实质性参考意义。
    **说明：**

    -   实时计算是分布式计算框架，以上3类延时指标的Metric，在计算Source的单个分区（Shard/Partition等）后，汇报所有分区中的最大值并呈现到前端页面上。因此，前端页面上显示的汇聚后的数据间隔时间并不精确等于业务延时–数据滞留时间。
    -   如果Source中的某个分区没有新的数据，将会导致业务延迟逐渐增大。
-   **各Source的TPS数据输入**

    对实时计算作业所有的流式数据输入进行统计，记录每秒读取数据源表的Block的数，让您直观地了解数据存储TPS（Transactions Per Second）的情况。与TPS不同，RPS（Record Per Second）是读取数据源TPS的Block数解析后的数据，单位是条/秒。例如，如果日志服务1秒读取5个LogGroup，则TPS=5。如果每个LogGroup解析出来8个日志记录，则一共解析出40个日志记录，RPS=40。

-   **各Sink的数据输出**

    统计实时计算作业所有的数据输出（并非是流式数据存储，而是全部数据存储），让您直观地了解数据存储RPS（Record Per Second）的情况。通常，在系统运维过程中，如果出现没有数据输出的情况，除了检查上游是否存在数据输入，也要检查下游是否真的存在数据输出。

-   **各Source的RPS数据输入**

    统计实时计算作业所有的流式数据输入，让您直观地了解数据存储RPS（Record Per Second）情况。通常，在系统运维过程中，如果出现没有数据输出的情况，需要查看该值，判断数据源输入数据是否存在异常。

-   **各Source的数据流量输入**

    统计实时计算作业所有的流式数据输入和每秒读取输入源表的流量，让您直观地了解数据流量BPS（Byte Per Second）情况。

-   **各Source的脏数据**

    显示实时计算Source各时间段脏数据条数。

-   **AutoScale的成功和失败数**

    显示AutoScale成功执行和未成功执行的次数。

    **说明：** 仅实时计算3.0.0以上版本支持此曲线。

-   **AutoScale使用的CPU**

    显示执行AutoScale时消耗的CPU数量。

    **说明：** 仅实时计算3.0.0以上版本支持此曲线。

-   **AutoScale使用的MEM**

    显示执行AutoScale时消耗的内存量。

    **说明：** 仅实时计算3.0.0以上版本支持此曲线。


## Advanced View

阿里云实时计算提供可以恢复数据流应用到一致状态的容错机制。容错机制的核心就是持续创建分布式数据流及其状态的一致快照。这些快照在系统遇到故障时，充当可以回退的一致性检查点（Checkpoint）。

分布式快照的核心概念之一就是数据栅栏（Barrier）。这些barrier被插入到数据流中，作为数据流的一部分和数据一起向下流动。Barrier不会干扰正常数据，数据流严格有序。一个Barrier把数据流分割成两部分：一部分进入到当前快照，另一部分进入下一个快照。每一个Barrier都带有快照ID，并且Barrier之前的数据都进入了此快照。Barrier不会干扰数据流处理，所以非常轻量。多个不同快照的多个Barrier会在流中同时出现，即多个快照可能同时被创建。

![](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/6321659951/p33962.png)

Barrier在数据源端插入，当Snapshot n的Barrier插入后，系统会记录当前Snapshot位置值n（用Sn表示），然后Barrier继续往下流动。当一个Operator从其输入流接收到所有标识Snapshot n的Barrier时，它会向其所有输出流插入一个标识Snapshot n的Barrier。当Sink Operator （DAG流的终点）从其输入流接收到所有Barrier n时，Operator向检查点协调器确认Snapshot n已完成。当所有Sink都确认了这个快照时，快照就被标识为完成。

![](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/6321659951/p33963.png)

以下是记录Checkpoint的各种参数配置。

|数据曲线名称|说明|
|------|--|
|Checkpoint Duration|进行Checkpoint保存状态所花费的时间，单位为毫秒。|
|Checkpoint Size|进行Checkpoint所消耗的内存大小。|
|Checkpoint Alignment Time|当前节点进行Checkpoint操作，等待上游所有节点到达当前节点的时间。当Sink Operator（DAG流的终点）从其输入流接收到所有Barrier n时，它会向Checkpoint Coordinator确认Snapshot n已完成。当所有Sink都确认了这个快照，快照就被标识为完成。这个等待的时间就是CheckpointAlignmentTime。|
|Checkpoint Count|指定时间段内Checkpoint的数量。|
|Get|指定时间段内每个SubTask对ROCKSDB进行Get操作所花费的时间（最大值）。|
|Put|指定时间段内每个SubTask对ROCKSDB进行Put操作所花费的时间（最大值）。|
|Seek|指定时间段内每个SubTask对ROCKSDB进行Seek操作所花费的时间（最大值）。|
|State Size|指定时间段内Job内部State存储大小（如果增量过快，Job是异常的）。|
|GMS GC Time|指定时间段内Job内部容器进行GC（Garbage Collection）花费的时间。|
|GMS GC Rate|指定时间段内Job内部容器进行GC（Garbage Collection）的频率。|

## WaterMark

|数据曲线名称|说明|
|------|--|
|Watermark Delay|WaterMark距离系统时间的差值。|
|数据迟到丢弃TPS|每秒丢弃晚于Watermark时间到达Window的数据量。|
|数据迟到累计丢弃数|累计丢弃晚于Watermark时间到达Window的数据量。|

## Delay

**Source SubTask 最大延迟 Top 15**

![](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/7321659951/p33978.png)

表示每个Source并发的业务延时的时长。

## Throughput

|数据曲线名称|说明|
|------|--|
|Task Input TPS|作业中所有的Task的数据的输入状况。|
|Task Output TPS|作业中所有的Task的数据的输出状况。|

## Queue

|数据曲线名称|说明|
|------|--|
|Input Queue Usage|作业中所有的Task的数据的输入队列。|
|Output Queue Usage|作业中所有的Task的数据的输出队列。|

## Tracing

|数据曲线名称|说明|
|------|--|
|Time Used In Processing Per Second|处理Task中每秒数据所花费的时长。|
|Time Used In Waiting Output Per Second|等待Task中每秒数据输出所花费的时长。|
|Task Latency Histogram Mean|每个Task的延时时长。|
|Wait Output Histogram Mean|每个Task的等待输出时长。|
|Wait Input Histogram Mean|每个Task的等待输入时长。|
|Partition Latency Mean|每个分区中并发的延时时长。|

## Process

|数据曲线名称|说明|
|------|--|
|Process Memory RSS|每个进程内存的使用状况。|
|CPU Usage|每个进程CPU的使用状况。|

## JVM

|数据曲线名称|说明|
|------|--|
|Memory Heap Used|Job使用的JVM Heap存储量。|
|Memory Non-Heap Used|Job使用的JVM 非Heap存储量。|
|Thread Count|Job的线程数。|
|GC\(CMS\)|Job完成GC（Garbage Collection）的次数。|

