---
keyword: [公网, 访问, OSSException, undefined]
---

# 常见问题

本文汇总了Flink全托管常见问题与解决方案，包括基本概念、作业开发问题、作业运维问题、网络相关问题、异常情况处理和Connector相关问题。

-   基本概念
    -   [什么是实时计算？](#section_aqx_epg_szw)
    -   [实时计算与批量计算相比存在哪些差异？](#section_bk9_c2g_dbd)
    -   [阿里云实时计算Flink版使用有哪些限制？](#section_9cb_700_837)
    -   [什么是流数据？](#section_u13_pne_ql6)
    -   [阿里云实时计算Flink版的数据存储有哪些类型？](#section_qe0_rxy_xgc)
-   作业开发问题
    -   [如何在OSS控制台上传JAR包？](#section_n50_cdr_610)
    -   [如何设置Checkpoint和State相关参数？](#section_hsq_3mv_ebg)
    -   [报错：undefined](#section_4ue_qt3_mk1)
    -   [报错：Object '\*\*\*\*' not found](#section_afx_xbz_ztk)
    -   [报错：Only a single 'INSERT INTO' is supported](#section_pay_6fz_jk7)
-   作业运维问题
    -   [如何查找引发告警的作业？](#section_cji_n5d_vu7)
    -   [如何查看工作空间ID等信息？](#section_39s_s0f_psa)
    -   [Python作业，如果Checkpoint慢怎么办？](#section_zga_zxt_01g)
    -   [报错：OSSException](#section_y7f_jj0_czt)
    -   [报错：exceeded quota: resourcequota](#section_a84_588_2z2)
    -   [报错：Exceeded checkpoint tolerable failure threshold](#section_ccg_u9w_7cn)
    -   [INFO：org.apache.flink.fs.osshadoop.shaded.com.aliyun.oss](#section_oi6_jz0_td1)
-   网络相关问题
    -   [Flink全托管集群如何访问公网？](#section_vvx_0qb_o7d)
    -   [如何访问跨VPC里的存储资源？](#section_uaq_oes_stp)
-   异常情况处理问题
    -   [如何解决Flink依赖冲突问题？](#section_tiq_eb4_sqz)
    -   [如何解析Flink作业所依赖服务的域名？](#section_k1k_0q4_kzg)
    -   [JobManager没有运行起来，如何快速定位问题？](#section_xr7_5g6_x7p)
    -   [不小心删除了角色或者变更了授权策略，导致Flink全托管服务不可用怎么办？](#section_2ph_vmw_pwl)
-   Connector相关问题
    -   Kafka源表
        -   [Flink如何获取JSON数据？](#section_jld_ni2_onv)
        -   [Flink中的Commit Offset有什么作用？](#section_uv0_731_j2e)
        -   [为什么Flink和Kafka之间的网络是连通的，但是依然会有timeout expired while fetching topic metadata的报错？](#section_hiy_fx2_uvi)
    -   DataHub源表
        -   [分裂或者缩容DataHub Topic后导致Flink作业失败，如何恢复？](#section_wyu_gz3_678)
        -   [可以删除正在消费的DataHub Topic吗？](#section_9cc_z2v_iwb)
    -   全量和增量MaxCompute源表
        -   [endPoint和tunnelEndpoint是指什么？如果配置错误会产生什么结果？](#section_jmb_qbv_7sg)
        -   [启动作业时出现Akka超时报错，但是全量MaxCompute源表和增量MaxCompute获取Metadata速率正常，应该如何处理？](#section_r9s_ja4_16k)
        -   [全量MaxCompute和增量MaxCompute是如何读取MaxCompute数据的？](#section_b0q_qrp_m4p)
        -   [引用MaxCompute作为数据源，在作业启动后，向已有的分区或者表里追加数据，这些新数据是否能被全量MaxCompute或增量MaxCompute源表读取？](#section_4ze_7mp_whs)
        -   [全量MaxCompute和增量MaxCompute源表作业是否支持暂停作业后修改并发数，再恢复作业？](#section_m5g_xb8_2yk)
        -   [作业启动位点设置了2019-10-11 00:00:00， 为什么启动位点前的分区也会被全量MaxCompute源表读取？](#section_vhg_dgl_rmk)
        -   [增量MaxCompute源表监听到新分区时，如果该分区还有数据没有写完，如何处理？](#section_f52_hsr_zyp)
        -   [报错：ErrorMessage=Authorization Failed \[4019\], You have NO privilege'ODPS:\*\*\*'](#section_947_odv_kbe)
    -   Mysql CDC源表
        -   [不支持定义Watermark，那如何进行窗口聚合？](#section_2n7_32h_0xy)
        -   [如何跳过Snapshot阶段，只从变更数据开始读取？](#section_sph_lqe_qob)
        -   [如何读取一个分库分表的MySQL数据库？](#section_97c_4ep_vw0)
        -   [全表读取阶段效率慢、存在反压，应该如何解决？](#section_afp_oeh_ys7)
    -   RDS MySQL结果表
        -   [Flink的结果数据写入RDS表，是按主键更新的，还是生成1条新的记录？](#section_bk2_s1b_xaj)
        -   [使用RDS表中的唯一索引进行GROUP BY时需要注意什么？](#section_2o1_y9h_n5k)
    -   ClickHouse结果表

        [ClickHouse结果表是否支持回撤更新数据？](#section_y9z_xyr_xby)

    -   Print结果表

        [如何在控制台查看print数据结果？](#section_874_qcs_m1g)

    -   Tablestore维表

        [维表进行JOIN时，如果查询不到数据，应该如何处理？](#section_ps0_xd6_gvr)

    -   MaxCompute维表

        [max\_pt\(\)和max\_pt\_with\_done\(\)的区别是什么？](#section_bj8_t1d_5el)


## 什么是实时计算？

数据的业务价值会随着时间的流失而迅速降低，因此在数据发生后必须尽快对其进行计算和处理。目前，对于信息的高时效性和可操作性要求越来越高，这就要求软件系统能够在更短的时间内处理更多的数据。而传统的大数据处理模型将在线事务处理和离线分析从时序上完全分开，对于数据加工均遵循传统的日清日毕模式，即以小时甚至以天为计算周期对当前数据进行累计并处理。显然，传统的大数据处理方式无法满足数据实时计算的需求。数据处理时延造成的影响对要求苛刻的业务场景会非常明显，例如实时大数据分析、风控预警、实时预测和金融交易等领域。

实时计算可以有效地缩短全链路数据流时延、实时化计算业务逻辑、平摊计算成本，最终有效满足实时处理大数据的业务需求。

实时计算具备三大特点：

-   实时（Realtime）且无界（Unbounded）的数据流

    实时计算面对的数据是实时且流式的，这些数据按照时间发生顺序被实时计算订阅和消费。例如网站的访问单击日志流，只要网站不关闭，其单击日志流将不停产生并进入实时计算系统。

-   持续（Continous）且高效的计算

    实时计算是一种事件触发的计算模式，触发源为无界流式数据。一旦有新的流数据进入实时计算系统，它就立刻发起并进行一次计算任务，因此整个过程是持续进行的。

-   流式（Streaming）且实时的数据集成

    被流数据触发的计算结果，可以被直接写入目的数据存储。例如将计算后的报表数据直接写入阿里云关系型数据库RDS（Relational Database Service）进行报表展示。因此流数据的计算结果可以同流式数据一样，持续被写入目的数据存储。


## 实时计算与批量计算相比存在哪些差异？

下面从用户和产品层面来理解两类计算方式的区别。

-   批量计算

    批量计算是一种批量、高时延、主动发起的计算。目前绝大部分传统数据计算和数据分析服务均基于批量数据处理模型，即使用ETL系统或者OLTP系统进行构造数据存储，在线的数据服务（包括Ad-Hoc查询、DashBoard等）通过构造SQL语言访问上述数据存储并取得分析结果。这套数据处理的方法论伴随着关系型数据库在工业界的演进而被广泛采用。传统的批量数据处理模型如下图所示。

    ![批量计算模型](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/9852749951/p81803.png)

    1.  装载数据

        对于批量计算，用户需要预先将数据加载到计算系统，您可以使用ETL系统或者OLTP系统装载原始数据。系统将根据自己的存储和计算情况，对于装载的数据进行一系列查询优化、分析和计算。

    2.  提交请求

        系统主动发起一个计算作业（例如MaxCompute的SQL作业，或Hive的SQL作业）并向上述数据系统进行请求。此时计算系统开始调度（启动）计算节点进行大量数据计算，该过程的计算量可能非常大，耗时长达数分钟乃至于数小时。由于数据累计处理不及时，上述计算过程中可能就会存在一些历史数据，导致数据不新鲜。

        **说明：** 您可以根据业务需要，随时调整SQL语句后，再次提交作业。您甚至可以使用AdHoc查询做到即时修改即时查询。

    3.  返回结果

        计算作业完成后将数据以结果集形式返回给用户，由于保存在数据计算系统中的计算结果数据量巨大，需要用户再次集成数据到其他系统。一旦数据结果巨大，整体的数据集成过程就会漫长，耗时可能长达数分钟乃至于数小时。

-   实时计算

    实时计算是一种持续、低时延、事件触发的计算作业。相对于批量计算，流式计算整体上还属于比较新颖的计算概念。由于当前实时计算的计算模型较为简单，所以在大部分大数据计算场景下，实时计算可以看做是批量计算的增值服务，实时计算更强调计算数据流和低时延。实时计算数据处理模型如下。

    ![流计算模型 ](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/9852749951/p81795.png)

    1.  实时数据流

        使用实时数据集成工具，将实时变化的数据传输到流式数据存储（例如消息队列、DataHub）。此时数据的传输实时化，将长时间累积的大量数据平摊到每个时间点，不停地小批量实时传输，因此数据集成的时延得以保证。

        源源不断的数据被写入流数据存储，不需要预先加载的过程。同时，流计算对于流式数据不提供存储服务，数据持续流动，在计算完成后就被立刻丢弃。

    2.  提交流式任务

        批量计算要等待数据集成全部就绪后才能启动计算作业，而流式计算作业是一种常驻计算服务。实时计算作业启动后，一旦有小批量数据进入流式数据存储，流计算会立刻计算并得出结果。同时，阿里云流计算还使用了增量计算模型，将大批量数据分批进行增量计算，进一步减少单次运算规模并有效降低整体运算时延。

        从用户角度，对于流式作业，必须预先定义计算逻辑，并提交到流式计算系统中。在整个运行期间，流计算作业逻辑不可更改。用户通过停止当前作业运行后再次提交作业，此时之前已经计算完成的数据是无法重新再次被计算。

    3.  实时结果流

        不同于批量计算，结果数据需等待数据计算结果完成后，批量将数据传输到在线系统。流式计算作业在每次小批量数据计算后，无需等待整体的数据计算结果，会立刻将数据结果投递到在线/批量系统，实现计算结果的实时化展现。

    使用实时计算的顺序如下：

    1.  提交实时计算作业。
    2.  等待流式数据触发实时计算作业。
    3.  计算结果持续不断对外写出。

计算模型差别对比，详情内容如下表所示。

|对比指标|批量计算|实时计算|
|----|----|----|
|数据集成方式|预先加载数据。|实时加载数据到实时计算。|
|使用方式|业务逻辑可以修改，数据可重新计算。|业务逻辑一旦修改，之前的数据不可重新计算（流数据易逝性）。|
|数据范围|对加载的所有或大部分数据进行查询或处理。|对滚动时间窗口内的数据或仅对最近的数据记录进行查询或处理。|
|数据大小|大批量数据。|单条记录或几条记录的微批量数据。|
|性能|几分钟至几小时的延迟。|大约几秒或几毫秒的延迟。|
|分析|复杂分析。|简单的响应函数、聚合和滚动指标。|

## 阿里云实时计算Flink版使用有哪些限制？

请您评估以下限制对您业务的影响，并做相应的调整。如有疑问，请您[提交工单](https://selfservice.console.aliyun.com/ticket/createIndex?accounttraceid=f7b76db740fa486baa4b63bd5848fbc1idrb)进行咨询。

-   阿里云实时计算Flink版购买指导，请参见[开通流程](/cn.zh-CN/Flink全托管/开通流程.md)。
-   阿里云实时计算Flink版WebConsole仅支持Chrome浏览器访问。
-   阿里云实时计算Flink版包年包月支持华北2（北京）、华东2（上海）、华东1（杭州）、华南1（深圳）、新加坡和中国（香港）地域，您可以按区域购买。
-   阿里云实时计算Flink版按量付费支持华南1（深圳）、华北2（北京）、华东2（上海）和华东1（杭州）地域，您可以按区域购买。
-   阿里云实时计算Flink版在简单的流式压测处理场景下（例如过滤、清洗等），一个实时计算CU的处理能力为1000条/秒。在复杂的流式压测处理场景下（例如复杂UDF计算、聚合操作等），一个实时计算CU的处理能力为500条/秒。

    **说明：** 请根据您的业务情况，评估所需的CU数量。

-   阿里云实时计算Flink版对整个下属的Task、Task版本、IDE打开Task页面均有不同限制，包括：
    -   一个阿里云账号可以购买多个项目。
    -   单个项目下最多允许创建100个文件夹，深度最大不超过5层。
    -   单个项目下最多允许上传50个UDF或JAR。
    -   单个文件夹内最大文件数为500，包含目录。

## 什么是流数据？

所有大数据的产生均可以看作是一系列离散事件，这些离散事件是一条条事件流或数据流。相对于离线数据，流数据的规模普遍较小。流数据是由数据源持续产生的数据，示例如下：

-   使用移动或Web应用程序生成的日志文件。
-   网购数据。
-   游戏内玩家活动信息。
-   社交网站信息。
-   金融交易大厅或地理空间服务数据中心内所连接设备或仪器的遥测数据。
-   地理空间服务信息。
-   设备或仪器的遥测数据。

## 阿里云实时计算Flink版的数据存储有哪些类型？

阿里云实时计算Flink版本身不带有业务存储，所有的数据均是来自于外部存储系统持有的数据。阿里云实时计算Flink版支持以下几类数据存储类型：

-   流式的数据输入：流式数据的输入会触发和推动实时计算Flink版进行数据处理。每个实时计算Flink版作业必须至少声明一个流式数据输入源。
-   静态数据输入：静态数据输入也被称为维表，对于每条流式数据，可以关联一个外部静态数据源进行查询，为实时计算Flink版提供数据关联查询。
-   结果表输出：实时计算Flink版将计算的结果数据写出到目的数据表，为下游数据继续消费提供各类读写接口。

## 如何在OSS控制台上传JAR包？

1.  在Flink全托管控制台上查看当前集群的OSS Bucket。

    ![OSS Bucket](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/6908031161/p186374.png)

    OSS Bucket信息如下图所示。

    ![Bucket详情](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/6908031161/p230660.png)

2.  登录[OSS管理控制台](https://oss.console.aliyun.com/)，上传JAR包至目标Bucket的/artifats/namespaces目录下。

    ![jar目录](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/9906276061/p186522.png)

3.  在Flink全托管控制台左侧导航栏，单击**资源上传**，查看通过OSS控制台上传的JAR包。

    ![查看jar包](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/0016276061/p186527.png)


## 如何设置Checkpoint和State相关参数？

在目标作业详情页面右上角，单击**编辑**后，在页面右侧**高级配置**面板的**更多Flink配置**中，添加代码后保存生效，代码示例如下。

![详情](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/6629484261/p182189.png)

Flink全托管在开源Flink的基础上，还可以设置如下表所示参数。

|参数|说明|备注|
|--|--|--|
|state.backend|流状态存储系统。|取值如下：-   GeminiStateBackend（默认值）：阿里自研的流状态存储系统。
-   RocksDB：开源Flink流状态存储系统。 |
|table.exec.state.ttl|SQL作业的State TTL（Time To Live）。|**说明：** State TTL默认值为空，即代表State不过期。推荐State TTL设置为12960000毫秒（1.5天）。 |
|state.backend.gemini.ttl.ms|Datestream和Python作业的State TTL（Time To Live）。|

开源Flink中的Checkpoint和State更多参数详情，请参见[Checkpoints and State Backends](https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/config/)。

## 报错：undefined

-   报错详情

    ![报错信息](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/8043449951/p141940.png)

-   报错原因

    您的JAR包较大。

-   解决方案

    您可以在[OSS管理控制台](https://oss.console.aliyun.com/)上传JAR包，详情请参见[如何在OSS控制台上传JAR包？](#section_n50_cdr_610)


## 报错：Object '\*\*\*\*' not found

-   报错详情

    单击**运行**后，验证报错详情如下。

    ![报错详情](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/6419898161/p266344.png)

-   报错原因

    在DDL和DML同在一个文本中提交运行时，DDL没有声明为CREATE TEMPORARY TABLE。

-   解决方案

    在DDL和DML同在一个文本中提交运行时，DDL需要声明为CREATE TEMPORARY TABLE，而不是声明为CREATE TABLE。


## 报错：Only a single 'INSERT INTO' is supported

-   报错详情

    单击**运行**后，验证报错详情如下。

    ![报错详情](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/6419898161/p266356.png)

-   报错原因

    多个DML语句没有写在关键语句`BEGIN STATEMENT SET;`和`END;`之间。

-   解决方案

    将多个DML语句写在`BEGIN STATEMENT SET;`和`END;`之间。详情请参见[INSERT INTO语句](/cn.zh-CN/Flink全托管/Flink SQL参考/DML语句/INSERT INTO语句.md)。


## 如何查找引发告警的作业？

告警事件中包含JobID和Deployment ID信息，但是由于作业Failover后导致JobID发生变化，因此您需要通过Deployment ID查找是哪个作业报错。目前，Flink全托管控制台上暂无Deployment ID信息，您需要在作业的URL中获取Deployment ID信息。

![Deployment ID](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/5908031161/p180106.png)

## 如何查看工作空间ID等信息？

在管理控制台目标工作空间名称右侧，选择**其他** \> **工作空间详情**。

![工作空间详情](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/6908031161/p183281.png)

## Python作业，如果Checkpoint慢怎么办？

-   问题原因

    Python算子内部有一定的缓存，在进行Checkpoint时，需要将缓存中的数据全部处理完。因此，如果Python UDF的性能较差，则会导致Checkpoint时间变长，从而影响作业执行。

-   解决方案

    在作业开发页面，单击右侧的**高级配置**后，在**更多Flink配置**项中，设置以下参数，将缓存调小。

    ```
    python.fn-execution.bundle.size：默认值为100000，单位是条数。
    python.fn-execution.bundle.time：默认值为1000，单位是毫秒。
    ```

    参数的详细信息请参见[Flink Python配置](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/python/python_config.html)。


## 报错：OSSException

-   报错详情

    新作业启动或已上线作业重启时报错。

    ![报错](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/8043449951/p147396.png)

-   报错原因

    对象存储OSS开启了版本控制，导致存在大量的Object。

-   解决方案
    1.  在[OSS管理控制台](https://oss.console.aliyun.com/)，关闭版本控制，详情请参见[版本控制相关操作](/cn.zh-CN/控制台用户指南/存储空间管理/冗余与容错/版本控制相关操作.md)。
    2.  将oss://\{bucket\}/flink-jobs/namespaces/\{namespace\}/jobs/下的Object全部删除。

        ```
        ./ossutil64 rm oss://{bucket}/flink-jobs/namespaces/{namespace}/jobs/ --all-versions -r
        ```

        **说明：**

        -   本文以[ossutil](/cn.zh-CN/常用工具/命令行工具ossutil/概述.md)工具为例，您也可以使用其他方式删除Object。
        -   命令详情参见[rm（删除）](/cn.zh-CN/常用工具/命令行工具ossutil/常用命令/rm（删除）.md)。
    3.  重启Flink全托管作业。

## 报错：exceeded quota: resourcequota

-   报错详情

    作业启动过程中报错。

    ![报错](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/6672094061/p180066.png)

-   报错原因

    当前项目空间资源不足导致作业启动失败。

-   解决方案

    您需要对项目资源进行资源变配，详情请参见[单项目资源变配](/cn.zh-CN/Flink全托管/项目管理.md)。


## 报错：Exceeded checkpoint tolerable failure threshold

-   报错详情

    作业运行过程中报错。

    ![详情](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/7685741261/p275538.png)

-   报错原因

    未设置任务允许Checkpoint失败的次数，系统默认Checkpoint失败一次就触发一次Failover。

-   解决方案
    1.  在作业开发页面右侧，单击**高级配置**。
    2.  在**更多Flink配置**文本框，输入如下参数。

        ```
        execution.checkpointing.tolerable-failed-checkpoints: num
        ```

        您需要设置num值来调整任务允许Checkpoint失败的次数。num需要为0或正整数。如果num为0时，则表示不允许存在任何Checkpoint异常或者失败。


## INFO：org.apache.flink.fs.osshadoop.shaded.com.aliyun.oss

-   报错详情

    ![报错详情](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/6210707161/p258496.png)

-   报错原因

    OSS每次创建新目录时，会先检查是否存在该目录，如果不存在，就会报这个INFO信息，但该INFO信息不影响Flink作业运行。

-   解决方案

    在日志模板中添加`<Logger level="ERROR" name="org.apache.flink.fs.osshadoop.shaded.com.aliyun.oss"/>`。详情请参见[配置作业日志](/cn.zh-CN/Flink全托管/运维管理/配置作业日志.md)。


## Flink全托管集群如何访问公网？

-   背景说明

    Flink全托管集群默认不具备访问公网的能力，但阿里云提供的NAT网关可以实现VPC网络与公网网络互通，以满足部分Flink全托管集群用户通过UDX或Datastream代码访问公网的需求。

    ![背景说明](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/5952749951/p86093.png)

-   解决方案

    通过在VPC中创建NAT网关，并创建SNAT条目，将Flink全托管集群所在的交换机绑定至弹性公网IP（EIP），即可通过EIP访问公网。具体配置方法请参见：

    1.  创建NAT网关，详情请参见[创建NAT网关](/cn.zh-CN/购买指南/购买NAT网关.md)。
    2.  创建SNAT条目，详情请参见[创建SNAT条目](/cn.zh-CN/基本功能操作/创建SNAT实现访问公网服务.md)。
    3.  将Flink全托管集群所在的交换机绑定至弹性公网IP（EIP），详情请参见[绑定EIP](/cn.zh-CN/基本功能操作/创建NAT网关实例.md)。

## 如何访问跨VPC里的存储资源？

您可以通过以下几种方式跨VPC访问存储资源：

-   [提交工单](https://selfservice.console.aliyun.com/ticket/createIndex?accounttraceid=f7b76db740fa486baa4b63bd5848fbc1idrb)，产品名称选择**VPC**，要求通过**高速通道**或其它产品建立网络连接，但是此种方式需要付费。
-   使用VPN网关建立VPC到VPC的VPN连接，详情请参见[建立VPC到VPC的连接](/cn.zh-CN/最佳实践/建立VPC到VPC的连接.md)。
-   退掉和Flink全托管不同VPC的存储服务后，重新购买一个与Flink全托管相同VPC的存储服务。
-   释放Flink全托管服务后，重新购买一个和存储服务相同VPC的Flink全托管服务。
-   开通Flink全托管的公网访问能力，通过公网访问存储服务。Flink全托管集群默认不具备访问公网的能力，如有需求，详情请参见[Flink全托管集群如何访问公网？](#section_vvx_0qb_o7d)。

    **说明：** 因为在延迟性方面，公网不如内网，如果您对性能有要求，建议不要使用此方式。


## 如何解决Flink依赖冲突问题？

-   问题现象
    -   有明显报错，且引发错误的为Flink或Hadoop相关类。

        ```
        java.lang.AbstractMethodError
        java.lang.ClassNotFoundException
        java.lang.IllegalAccessError
        java.lang.IllegalAccessException
        java.lang.InstantiationError
        java.lang.InstantiationException
        java.lang.InvocationTargetException
        java.lang.NoClassDefFoundError
        java.lang.NoSuchFieldError
        java.lang.NoSuchFieldException
        java.lang.NoSuchMethodError
        java.lang.NoSuchMethodException
        ```

    -   无明显报错，但会引起一些不符合预期的现象，例如：
        -   日志不输出或log4j配置不生效。

            该类问题通常是由于依赖中携带了log4j相关配置导致的。需要排查作业JAR包中是否引入了log4j配置的依赖，可以通过在dependency中配置exclusions的方式去掉log4j配置。

            **说明：** 如果必须要使用不同版本的log4j，需要使用maven-shade-plugin将log4j相关的class relocation掉。

        -   RPC调用异常。

            Flink的Akka RPC调用出现依赖冲突可能导致的异常，默认不会显示在日志中，需要开启Debug日志进行确认。

            例如，Debug日志中出现`Cannot allocate the requested resources. Trying to allocate ResourceProfile{xxx}`，但是JM日志在`Registering TaskManager with ResourceID xxx`后，没有下文，直到资源请求超时报错`NoResourceAvailableException`。此外TM持续报错`Cannot allocate the requested resources. Trying to allocate ResourceProfile{xxx}`。

            原因：开启Debug日志后，发现RPC调用报错`InvocationTargetException`，该报错导致TM Slot分配到一半失败出现状态不一致，RM持续尝试分配Slot失败无法恢复。

-   问题原因
    -   作业JAR包中包含了不必要的依赖（例如基本配置、Flink、Hadoop和log4j依赖），造成依赖冲突从而引发各种问题。
    -   作业需要的Connector对应的依赖未被打入JAR包中。
-   排查方法
    -   查看作业pom.xml文件，判断是否存在不必要的依赖。
    -   通过`jar tf foo.jar`命令查看作业JAR包内容，判断是否存在引发依赖冲突的内容。
    -   通过`mvn dependency:tree`命令查看作业的依赖关系，判断是否存在冲突的依赖。
-   解决方案
    -   基本配置建议将scope全部设置为provided，即不打入作业JAR包。
        -   DataStream Java

            ```
            <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-streaming-java_2.11</artifactId>
              <version>${flink.version}</version>
              <scope>provided</scope>
            </dependency>
            ```

        -   DataStream Scala

            ```
            <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-streaming-scala_2.11</artifactId>
              <version>${flink.version}</version>
              <scope>provided</scope>
            </dependency>
            ```

        -   DataSet Java

            ```
            <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-java</artifactId>
              <version>${flink.version}</version>
              <scope>provided</scope>
            </dependency>
            ```

        -   DataSet Scala

            ```
            <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-scala_2.11</artifactId>
              <version>${flink.version}</version>
              <scope>provided</scope>
            </dependency>
            ```

    -   添加作业需要的Connector对应的依赖，并将scope全部设置为compile（默认的scope是compile），即打入作业JAR包中，以Kafka Connector为例，代码如下。

        ```
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-kafka_2.11</artifactId>
            <version>${flink.version}</version>
        </dependency>
        ```

    -   其他Flink、Hadoop和log4j依赖不建议添加。但是：
        -   如果作业本身存在基本配置或Connector相关的直接依赖，建议将scope设置为provided，示例如下。

            ```
            <dependency>
                <groupId>org.apache.hadoop</groupId>
                <artifactId>hadoop-common</artifactId>
                <scope>provided</scope>
            </dependency>
            ```

        -   如果作业存在基本配置或Connector相关的间接依赖，建议通过exclusion将依赖去掉，示例如下。

            ```
            <dependency>
                <groupId>foo</groupId>
                  <artifactId>bar</artifactId>
                  <exclusions>
                    <exclusion>
                    <groupId>org.apache.hadoop</groupId>
                    <artifactId>hadoop-common</artifactId>
                   </exclusion>
                </exclusions>
            </dependency>
            ```


## 如何解析Flink作业所依赖服务的域名？

如果您自建的Flink作业中所依赖的服务填写的是域名，则迁移上云至Flink全托管服务时，可能会报无法解析域名的问题。此时，您可以按照以下方案解析Flink作业所依赖服务的域名：

-   您已有自建的DNS，并且Flink VPC能够连通该自建DNS服务，且该自建DNS能够正常解析域名。

    此时，您可以基于Flink全托管作业模板进行域名解析。假如您的自建DNS IP为192.168.0.1，操作步骤如下：

    1.  登录[实时计算管理控制台](https://realtime-compute.console.aliyun.com/regions/cn-shanghai)。
    2.  在**Flink全托管**页签，单击目标工作空间**操作**列下的**控制台**。
    3.  在左侧导航栏，选择**系统管理** \> **作业模板**。
    4.  在**Flink配置**文本框，添加如下代码。

        ```
        env.java.opts: >-
          -Dsun.net.spi.nameservice.provider.1=default
          -Dsun.net.spi.nameservice.provider.2=dns,sun
          -Dsun.net.spi.nameservice.nameservers=192.168.0.1
        ```

        **说明：** 如果您的自建DNS有多个IP，建议IP之间使用英文逗号（,）分隔。

    5.  在Flink全托管控制台新建作业并运行。

        如果还报unknowhost错误，则意味着无法解析域名，请您[提交工单](https://selfservice.console.aliyun.com/ticket/createIndex?accounttraceid=f7b76db740fa486baa4b63bd5848fbc1idrb)进行咨询。

-   您没有自建的DNS或者Flink VPC无法与自建的DNS连通。

    此时，您需要基于阿里云云解析PrivateZone进行域名解析。假如您的Flink VPC为vpc-flinkxxxxxxx，Flink作业需要访问的服务的域名分别为aaa.test.com 127.0.0.1、bbb.test.com 127.0.0.2和ccc.test.com 127.0.0.3，操作步骤如下：

    1.  开通云解析PrivateZone。详情请参见[开通云解析PrivateZone](https://help.aliyun.com/document_detail/64627.html?spm=a2c4g.11186623.6.553.16ee82d2CHmY73)
    2.  添加Zone，以Flink作业需要访问服务的公共后缀作为Zone名称。详情请参见[添加Zone](https://help.aliyun.com/document_detail/64628.html?spm=a2c4g.11186623.6.554.552f653aT2zn3D)。
    3.  关联Flink全托管的VPC。详情请参见[关联/解关联VPC](https://help.aliyun.com/document_detail/64629.html?spm=a2c4g.11186623.6.555.6a293eefl8qHVa)
    4.  添加解析记录至Zone。详情请参见[添加PrivateZone解析记录](https://help.aliyun.com/document_detail/64635.html?spm=a2c4g.11186623.6.560.395b82d2p682dR)。

        ![结果](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/1944772261/p280368.png)

    5.  在Flink全托管控制台新建作业并运行或者停止后再启动历史作业 。

        如果还报unknowhost错误，则意味着无法解析域名，请您[提交工单](https://selfservice.console.aliyun.com/ticket/createIndex?accounttraceid=f7b76db740fa486baa4b63bd5848fbc1idrb)进行咨询。


## JobManager没有运行起来，如何快速定位问题？

JobManager没有运行起来即无法进入**Flink UI**页面。此时，您可以通过以下操作进行问题定位：

1.  在**作业运维**页面，单击目标作业名称。
2.  单击**运行事件**页签。
3.  通过快捷键搜索error，获取异常信息。

    -   Windows系统：Ctrl+F
    -   Mac系统：Command+F
    ![示例](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/8797630261/p269255.png)


## 不小心删除了角色或者变更了授权策略，导致Flink全托管服务不可用怎么办？

-   方式一：创建AliyunStreamAsiDefaultRole角色后，添加AliyunStreamAsiDefaultRolePolicy0、AliyunStreamAsiDefaultRolePolicy1和FlinkServerlessPolicy自定义授权策略，实现重新授权。操作步骤详情请参见[手动授权（方式一）](/cn.zh-CN/Flink全托管/准备工作/角色授权.md)。
-   方式二：先删除资源编排服务ROS的资源栈、RAM角色和RAM权限策略后，登录实时计算管理控制台进行自动化重新授权。操作步骤详情请参见[手动授权（方式二）](/cn.zh-CN/Flink全托管/准备工作/角色授权.md)。

## Flink如何获取JSON数据？

-   如果您需要获取普通JSON数据，方法详情请参见[JSON Format](https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/formats/json.html)。
-   如果您需要获取嵌套的JSON数据，则源表DDL中使用ROW格式定义JSON Object，结果表DDL中定义好要获取的JSON数据对应的Key，在DML语句中设置好Key获取的方式，就可以获取到对应的嵌套Key的Value值。代码示例如下：
    -   测试数据

        ```
        {
            "a":"abc",
            "b":1,
            "c":{
                "e":["1","2","3","4"],
                "f":{"m":"567"}
            }
        }
        ```

    -   源表DDL定义

        ```
        CREATE TEMPORARY TABLE `kafka_table` (
          `a` VARCHAR,
           b int,
          `c` ROW<e ARRAY<VARCHAR>,f ROW<m VARCHAR>>  --c是一个JSON Object，对应Flink里面是ROW；e是json list，对应ARRAY。
        ) WITH (
          'connector' = 'kafka',
          'topic' = 'xxx',
          'properties.bootstrap.servers' = 'xxx',
          'properties.group.id' = 'xxx',
          'format' = 'json',
          'scan.startup.mode' = 'xxx'
        );
        ```

    -   结果表DDL定义

        ```
        CREATE TEMPORARY TABLE `sink` (
         `a` VARCHAR,
          b INT,
          e VARCHAR,
          `m` varchar
        ) WITH (
          'connector' = 'print',
          'logger' = 'true'
        );
        ```

    -   DML语句

        ```
        INSERT INTO `sink`
          SELECT 
          `a`,
          b,
          c.e[1], --Flink从1开始遍历数组，本示例为获取数组中的元素1。如果获取整个数组，则去掉[1]。
          c.f.m
        FROM `kafka_table`;
        ```

    -   测试结果

        ![测试结果](https://static-aliyun-doc.oss-accelerate.aliyuncs.com/assets/img/zh-CN/4107476261/p296378.png)


## Flink中的Commit Offset有什么作用？

Flink在每次Checkpoint成功时，才会向Kafka提交当前读取Offset。如果未开启Checkpoint，或者Checkpoint设置的间隔过大，在Kafka端可能会查询不到当前读取的Offset。

## 为什么Flink和Kafka之间的网络是连通的，但是依然会有timeout expired while fetching topic metadata的报错？

Flink和Kafka之间的网络连通并不意味着能读取数据，只有Kafka Broker在bootstrap过程中返回的集群metadata中描述的Endpoint， 才可以连通Flink和Kafka，并读取到Kafka的数据，详情请参见[Flink-cannot-connect-to-Kafka](https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/)。检查办法为：

1.  使用zkCli.sh或者zookeeper-shell.sh工具登录Kafka使用的Zookeeper。
2.  执行`ls /brokers/ids`命令列出所有的Kafka Broker ID。
3.  使用`get /brokers/ids/{your_broker_id}`命令查看Broker metadata信息。

    Endpoint信息显示在listener\_security\_protocol\_map中。

4.  确认Flink是否可以连通该Endpoint。

    如果该Endpoint中使用了域名，请为Flink配置对应的域名解析服务。


## 分裂或者缩容DataHub Topic后导致Flink作业失败，如何恢复？

如果分裂或者缩容了Flink正在读取的某个Topic，则会导致任务持续出错，无法自行恢复。该情况下需要重新启动（先停止再启动）来使任务恢复正常。

## 可以删除正在消费的DataHub Topic吗？

不支持删除或重建正在消费的DataHub Topic。

## endPoint和tunnelEndpoint是指什么？如果配置错误会产生什么结果？

endPoint和tunnelEndpoint参数说明参见[Endpoint](/cn.zh-CN/准备工作/Endpoint.md)。VPC环境中这两个参数如果配置错误可能会导致任务异常，异常情况详情如下：

-   如果endPoint配置错误，则任务上线停滞在91%的进度。
-   如果tunnelEndpoint配置错误，则任务运行失败。

## 启动作业时出现Akka超时报错，但是全量MaxCompute源表和增量MaxCompute获取Metadata速率正常，应该如何处理？

请合并小文件，具体步骤请参见文档[MaxCompute什么情况下会产生小文件？如何解决小文件问题？](/cn.zh-CN/常见问题/优化诊断.md)。

## 全量MaxCompute和增量MaxCompute是如何读取MaxCompute数据的？

全量MaxCompute和增量MaxCompute是通过Tunnel读取MaxCompute数据的，读取速度受MaxCompute Tunnel带宽限制。

## 引用MaxCompute作为数据源，在作业启动后，向已有的分区或者表里追加数据，这些新数据是否能被全量MaxCompute或增量MaxCompute源表读取？

启动Flink作业后，如果正在被Source读取或已经被Source读取完成的表或分区有新的数据追加，则这部分数据不会被读取，而且可能导致作业Failover。

全量MaxCompute和增量MaxCompute源表均使用`ODPS DOWNLOAD SESSION`读取表数据或者分区数据。新建`DOWNLOAD SESSION`，服务端会创建一个Index文件，相当于创建`DOWNLOAD SESSION`一瞬间数据的映射，后续的数据读取以这个映射为基础。因此在新建 `DOWNLOAD SESSION` 后，追加到MaxCompute表或者分区里的数据，正常流程下是不会被读取的。但如果MaxCompute源表中写入新数据，则会出现两种异常：

-   产生报错：如果Tunnel在读取数据的过程中写入新数据，则会产生报错`ErrorCode=TableModified,ErrorMessage=The specified table has been modified since the download initiated.`。
-   数据正确性无法保证：如果在Tunnel已经关闭后写入新数据，则这些数据不会被读取。但当作业发生Failover或者暂停后恢复作业，已经读过的数据可能会被重读，新写入的数据可能被读不全。

## 全量MaxCompute和增量MaxCompute源表作业是否支持暂停作业后修改并发数，再恢复作业？

MaxCompute源表暂时不支持暂停作业后修改并发数，再恢复作业。系统根据并发数，计算每个并发读取哪些分区的哪些数据。此外，每个并发会把消费情况记录至State，以便暂停恢复后或者Failover后，系统可以从上次读取的位置继续读取数据。

如果暂停作业后，修改了全量MaxCompute和增量MaxCompute源表的并发数后再恢复作业，会对作业产生无法预估的影响。因为已经读取的数据可能会被再次读取，没有读的数据可能会被遗漏。

## 作业启动位点设置了`2019-10-11 00:00:00`， 为什么启动位点前的分区也会被全量MaxCompute源表读取？

设置启动位点只对消息队列（例如DataHub）类型的数据源有效，对MaxCompute源表无效。Flink作业启动后数据读取的范围如下：

-   分区表：读取当前所有分区。
-   非分区表：读取当前存在的数据。

## 增量MaxCompute源表监听到新分区时，如果该分区还有数据没有写完，如何处理？

没有机制可以标志一个分区的数据是否已经完整。目前只要监听到新分区，就会读入。用增量MaxCompute源表读取一个MaxCompute分区表T，分区列是ds，推荐的写入方法为：不创建分区，先执行Insert overwrite table T partition \(ds='20191010'\) ...语句，作业结束且成功后，分区和数据一起可见。

**说明：** 不允许的写入方法为：先创建好分区，例如ds=20191010，再往分区里写数据。增量MaxCompute源表监听到新分区ds=20191010，立刻读入，如果此时该分区还有数据没有写完，就会漏读数据。

## 报错：ErrorMessage=Authorization Failed \[4019\], You have NO privilege'ODPS:\*\*\*'

-   报错详情

    作业运行过程中会在Failover页面或TaskManager.log页面报错，报错信息如下。

    ```
    ErrorMessage=Authorization Failed [4019], You have NO privilege'ODPS:***'
    ```

-   报错原因

    MaxCompute DDL定义中填写的用户身份信息无法访问MaxCompute。

-   解决方案

    通过阿里云账号、RAM用户账号或RAM角色认证用户身份，详情请参见[用户认证](/cn.zh-CN/安全管理/安全管理详解/用户及授权管理/用户认证.md)。如果您有其他疑问，请[提交工单](https://selfservice.console.aliyun.com/ticket/createIndex?accounttraceid=f7b76db740fa486baa4b63bd5848fbc1idrb)咨询，产品名称选择为MaxCompute。


## 不支持定义Watermark，那如何进行窗口聚合？

如果您需要在MySQL CDC源表上进行窗口聚合，可以考虑采用非窗口聚合的方式，即将时间字段转换成窗口值，然后根据窗口值进行GROUP BY聚合。例如，统计每个店铺每分钟的订单数和销量，代码示例如下。

```
SELECT shop_id, DATE_FORMAT(order_ts, 'yyyy-MM-dd HH:mm'), COUNT(*), SUM(price) FROM order_mysql_cdc GROUP BY shop_id, DATE_FORMAT(order_ts, 'yyyy-MM-dd HH:mm')
```

## 如何跳过Snapshot阶段，只从变更数据开始读取？

可以通过WITH参数debezium.snapshot.mode来控制，您可以设置为：

-   never：在启动时，不读取数据库的快照，而是直接从变更的最开始位置读取。但需要注意MySQL的变更旧数据可能会被自动清理，因此不能保证变更数据中包含了全量的数据，读取的数据不完整。
-   schema\_only：如果你不需要保证数据的一致性，只关心作业启动后数据库的新增变更，则可以设置为schema\_only，仅快照schema，不快照数据，从变更的最新数据开始读取。

## 如何读取一个分库分表的MySQL数据库？

如果MySQL是一个分库分表的数据库，分成了user\_00、user\_02和user\_99等多个表，且所有表的schema一致。则可以通过table-name选项，指定一个正则表达式来匹配读取多张表，例如设置table-name为user\_.\*，监控所有user\_前缀的表。database-name选项也支持该功能，但需要所有的表schema一致。

## 全表读取阶段效率慢、存在反压，应该如何解决？

可能是下游节点处理太慢导致反压了。因此您需要先排查下游节点是否存在反压。如果存在，则需要先解决下游节点的反压问题。您可以通过以下方式处理：

-   增加并发数。
-   开启minibatch等聚合优化参数（下游聚合节点）。

## Flink的结果数据写入RDS表，是按主键更新的，还是生成1条新的记录？

如果在DDL中定义了主键，会采用`INSERT INTO tablename(field1,field2, field3, ...) VALUES(value1, value2, value3, ...) ON DUPLICATE KEY UPDATE field1=value1,field2=value2, field3=value3, ...;`的方式更新记录，即对于不存在的主键字段会直接插入，存在的主键字段则更新相应的值。如果DDL中没有声明PRIMARY KEY，则会用`insert into`方式插入记录，追加数据。

## 使用RDS表中的唯一索引进行GROUP BY时需要注意什么？

-   需要在作业中的GROUP BY中声明该唯一索引。
-   RDS中只有一个自增主键，Flink作业中不能声明为PRIMARY KEY。

## ClickHouse结果表是否支持回撤更新数据？

当Flink结果表的DDL上指定了Primary Key时，则支持回撤更新数据。因为ClickHouse是一个用于联机分析（OLAP）的列式数据库管理系统，对UPDATE和DELETE的支持不够完善。如果在Flink DDL上指定了Primary Key，则尝试使用ALTER TABLE UPDATE和DELETE来更新和删除数据，因此会造成性能显著下降。

## 如何在控制台查看print数据结果？

查看print数据结果的步骤，有以下两种方式：

-   在Flink开发控制台查看：
    1.  在目标作业**Task Manager**页签，单击**Path, ID**。
    2.  在**logs**页签，查看print数据结果。
-   跳转到Flink UI界面查看：
    1.  在目标作业**作业总览**页签，单击**Flink UI**。
    2.  单击**Task Managers**。
    3.  单击**Path, ID**。
    4.  在**logs**页签，查看print数据结果。

## 维表进行JOIN时，如果查询不到数据，应该如何处理？

检查DDL语句和物理表中的Schema类型和名称是否一致。

## max\_pt\(\)和max\_pt\_with\_done\(\)的区别是什么？

max\_pt\(\)选取的是所有分区中字典序最大的分区。max\_pt\_with\_done\(\)选取的是所有分区中字典序最大，且伴随有`.done`分区的分区。如果分区列表示例如下：

-   ds=20190101
-   ds=20190101.done
-   ds=20190102
-   ds=20190102.done
-   ds=20190103

则`max_pt()`和`max_pt_with_done()`的区别如下：

-   ``partition`='max_pt_with_done()'`匹配的分区是`ds=20190102`。
-   ``partition`='max_pt()'`，匹配的分区是`ds=20190103`。

